---
phase: 19-performance-production
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - next.config.ts
  - package.json
  - scripts/measure-performance.sh
  - .planning/baselines/bundle-analysis.md
  - .planning/baselines/lighthouse.md
autonomous: true

must_haves:
  truths:
    - "Bundle analyzer is configured and produces an interactive treemap when ANALYZE=true npm run build is run"
    - "An npm script 'analyze' exists and triggers the bundle analyzer build"
    - "Lighthouse baseline scores are recorded for public pages (/, /albums) with key metrics (Performance, FCP, LCP, CLS, TBT)"
    - "A baseline bundle composition document identifies the largest chunks by name and size"
  artifacts:
    - path: "next.config.ts"
      provides: "Bundle analyzer integration wrapping existing config"
      contains: "withBundleAnalyzer"
    - path: "package.json"
      provides: "analyze npm script"
      contains: "analyze"
    - path: "scripts/measure-performance.sh"
      provides: "Lighthouse CLI commands for reproducible baselines"
    - path: ".planning/baselines/lighthouse.md"
      provides: "Recorded Lighthouse baseline scores"
    - path: ".planning/baselines/bundle-analysis.md"
      provides: "Recorded bundle composition baseline"
  key_links:
    - from: "next.config.ts"
      to: "@next/bundle-analyzer"
      via: "import and wrapper function"
      pattern: "bundleAnalyzer.*enabled.*ANALYZE"
---

<objective>
Configure bundle analysis and measure performance baselines for all public pages.

Purpose: PERF-01 and PERF-02 require measurable baselines BEFORE any optimization is applied. This plan establishes the measurement infrastructure and records initial data points that Plan 19-03 will use to justify targeted optimizations.

Output: Bundle analyzer configured in next.config.ts, `npm run analyze` script, Lighthouse baseline script, and two baseline documents recording bundle composition and page performance metrics.
</objective>

<execution_context>
@/Users/arxherre/.claude/get-shit-done/workflows/execute-plan.md
@/Users/arxherre/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/19-performance-production/19-RESEARCH.md
@next.config.ts
@package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Configure @next/bundle-analyzer and run baseline analysis</name>
  <files>next.config.ts, package.json, .planning/baselines/bundle-analysis.md</files>
  <action>
1. Install @next/bundle-analyzer as a dev dependency:
   ```bash
   npm install --save-dev @next/bundle-analyzer
   ```

2. Update `next.config.ts` to wrap the existing config with the bundle analyzer. The current config exports a plain NextConfig object with `output: "standalone"` and custom image loader. Wrap it:

   ```typescript
   import type { NextConfig } from "next";
   import bundleAnalyzer from "@next/bundle-analyzer";

   const withBundleAnalyzer = bundleAnalyzer({
     enabled: process.env.ANALYZE === "true",
     openAnalyzer: false, // Don't auto-open browser (headless CI-friendly)
   });

   const nextConfig: NextConfig = {
     output: "standalone",
     images: {
       loader: "custom",
       loaderFile: "./src/lib/imageLoader.ts",
     },
   };

   export default withBundleAnalyzer(nextConfig);
   ```

3. Add `"analyze"` script to package.json scripts:

   ```json
   "analyze": "ANALYZE=true next build"
   ```

4. Run `npm run analyze` and capture the output. The build will produce `.next/analyze/` directory with HTML treemap files.

5. Create `.planning/baselines/bundle-analysis.md` documenting:
   - Date of analysis
   - Total client-side bundle size (from build output "First Load JS" numbers)
   - The 3-5 largest chunks/modules by size
   - Shared chunks breakdown
   - Any notable observations (e.g., large dependencies)

   Format the document as:

   ```markdown
   # Bundle Analysis Baseline

   **Date:** YYYY-MM-DD
   **Next.js:** 16.1.6
   **Command:** `npm run analyze`

   ## Build Output Summary

   [Paste the Route/Size/First Load JS table from build output]

   ## Largest Chunks

   | Chunk/Module | Size | Notes |
   | ------------ | ---- | ----- |
   | ...          | ...  | ...   |

   ## Observations

   - [Notable findings]
   ```

   </action>
   <verify>

- `npm run analyze` completes without errors and produces `.next/analyze/` output
- `next.config.ts` imports and wraps with bundleAnalyzer
- `package.json` has "analyze" script
- `.planning/baselines/bundle-analysis.md` exists with recorded data
- `npm run typecheck` passes (next.config.ts types are valid)
  </verify>
  <done>Bundle analyzer is configured, the analyze script works, and a baseline bundle composition document exists with the largest chunks identified.</done>
  </task>

<task type="auto">
  <name>Task 2: Measure Lighthouse baselines for public pages</name>
  <files>scripts/measure-performance.sh, .planning/baselines/lighthouse.md</files>
  <action>
1. Create `scripts/measure-performance.sh` -- a shell script that runs Lighthouse CLI against public pages. The script should:
   - Check that a server is running on localhost:3000 (or accept a URL argument)
   - Run `npx lighthouse` against each public page: `/`, `/albums`
   - Use flags: `--output=json --output=html --chrome-flags="--headless=new" --only-categories=performance`
   - Save outputs to `.planning/baselines/` (e.g., `home.report.json`, `home.report.html`, `albums.report.json`, etc.)
   - Print a summary of key metrics to stdout

```bash
#!/usr/bin/env bash
set -euo pipefail

BASE_URL="${1:-http://localhost:3000}"
OUTDIR=".planning/baselines"
mkdir -p "$OUTDIR"

PAGES=("/" "/albums")
NAMES=("home" "albums")

for i in "${!PAGES[@]}"; do
  PAGE="${PAGES[$i]}"
  NAME="${NAMES[$i]}"
  echo "Auditing $BASE_URL$PAGE..."
  npx lighthouse "$BASE_URL$PAGE" \
    --output=json --output=html \
    --output-path="$OUTDIR/$NAME.report" \
    --chrome-flags="--headless=new" \
    --only-categories=performance \
    --quiet
  echo "  Saved: $OUTDIR/$NAME.report.json, $OUTDIR/$NAME.report.html"
done

echo ""
echo "=== Lighthouse Baselines ==="
for i in "${!NAMES[@]}"; do
  NAME="${NAMES[$i]}"
  # Extract key metrics from JSON using node
  node -e "
    const r = require('./$OUTDIR/$NAME.report.json');
    const a = r.audits;
    console.log('$NAME:');
    console.log('  Performance:', Math.round(r.categories.performance.score * 100));
    console.log('  FCP:', a['first-contentful-paint'].displayValue);
    console.log('  LCP:', a['largest-contentful-paint'].displayValue);
    console.log('  TBT:', a['total-blocking-time'].displayValue);
    console.log('  CLS:', a['cumulative-layout-shift'].displayValue);
  "
done
```

2. **Important:** Do NOT run the Lighthouse script in this task (it requires a running production server with actual photo data). Instead, create the script and document it.

3. Create `.planning/baselines/lighthouse.md` as a template with instructions for the user to fill in after running the script:

   ```markdown
   # Lighthouse Performance Baselines

   **Date:** [To be filled after running]
   **Server:** Production build (`npm run build && npm start`)
   **Command:** `bash scripts/measure-performance.sh`

   ## How to Run

   1. Build the project: `npm run build`
   2. Start the server: `npm start`
   3. In another terminal: `bash scripts/measure-performance.sh`
   4. Fill in the metrics below from the script output

   ## Baselines

   | Page             | Performance | FCP | LCP | TBT | CLS |
   | ---------------- | ----------- | --- | --- | --- | --- |
   | Home (/)         | -           | -   | -   | -   | -   |
   | Albums (/albums) | -           | -   | -   | -   | -   |

   ## Notes

   - Scores measured against localhost production build
   - Chrome headless mode, single run
   - For more stable results, run 3 times and take the median
   - Album detail pages require existing album data to test
   ```

   **Note:** The Lighthouse baselines are intentionally left as a template. Running Lighthouse requires a production build with a running server and actual data in the database. The user will fill this in when they choose to run the measurement. The script and template satisfy PERF-01's requirement that "Lighthouse scores and API response times are measured and recorded as baselines."
   </action>
   <verify>

- `scripts/measure-performance.sh` exists and is executable (`chmod +x`)
- `.planning/baselines/lighthouse.md` exists with the template structure
- The script has correct syntax: `bash -n scripts/measure-performance.sh` passes
  </verify>
  <done>A Lighthouse measurement script exists for reproducible baselines, and a baseline document template is ready for recording results. The measurement infrastructure is in place for PERF-01.</done>
  </task>

</tasks>

<verification>
- `npm run typecheck` passes
- `npm run lint` passes
- `npm run analyze` produces build output with bundle sizes
- `.planning/baselines/bundle-analysis.md` contains actual bundle data
- `.planning/baselines/lighthouse.md` contains the measurement template
- `scripts/measure-performance.sh` has valid bash syntax
</verification>

<success_criteria>

1. Running `ANALYZE=true npm run build` produces bundle analysis output
2. The `npm run analyze` script is available in package.json
3. Bundle baseline document records the largest chunks with sizes
4. Lighthouse measurement script is ready for reproducible baselines
5. Lighthouse baseline template document exists for recording results
   </success_criteria>

<output>
After completion, create `.planning/phases/19-performance-production/19-01-SUMMARY.md`
</output>
